{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "from constants import *\n",
    "\n",
    "from keras.models import model_from_json#load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import argparse\n",
    "import tensorflow\n",
    "#from models import *\n",
    "#from prepare_data import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import spacy\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(dropout_rate, model_weights_filename):\n",
    "    print \"Creating Model...\"\n",
    "    metadata = get_metadata()\n",
    "    num_classes = len(metadata['ix_to_ans'].keys())\n",
    "    num_words = len(metadata['ix_to_word'].keys())\n",
    "\n",
    "    embedding_matrix = prepare_embeddings(num_words, 300, metadata) #embedding_dimesion_choose = 300\n",
    "    model = vqa_model(embedding_matrix, num_words, 300, 26, dropout_rate, num_classes)\n",
    "    if os.path.exists(model_weights_filename):\n",
    "        print \"Loading Weights...\"\n",
    "        model.load_weights(model_weights_filename)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_limit,epoch,batch_size):\n",
    "    dropout_rate = 0.5\n",
    "    train_X, train_y = read_data(data_limit)    \n",
    "    model = get_model(dropout_rate,'/home/tushar/Desktop/Python/VQA/model_weights.h5') #Model_weight\n",
    "    checkpointer = ModelCheckpoint(filepath='/home/tushar/Desktop/Python/VQA/model_check.ckpt',verbose=1) #ModelCheckPoint\n",
    "    model.fit(train_X, train_y, epochs=epoch, batch_size=batch_size, callbacks=[checkpointer], shuffle=\"batch\")\n",
    "    model.save_weights('/home/tushar/Desktop/Python/VQA/model_weights.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val():\n",
    "    val_X, val_y, multi_val_y = get_val_data() \n",
    "    model = get_model(0.0,'/home/tushar/Desktop/Python/VQA/model_weights.h5')  #Model_weight\n",
    "    print \"Evaluating Accuracy on validation set:\"\n",
    "    metric_vals = model.evaluate(val_X, val_y)\n",
    "    print \"\"\n",
    "    for metric_name, metric_val in zip(model.metrics_names, metric_vals):\n",
    "        print metric_name, \" is \", metric_val\n",
    "\n",
    "    # Comparing prediction against multiple choice answers\n",
    "    true_positive = 0\n",
    "    preds = model.predict(val_X)\n",
    "    pred_classes = [np.argmax(_) for _ in preds]\n",
    "    for i, _ in enumerate(pred_classes):\n",
    "        if _ in multi_val_y[i]:\n",
    "            true_positive += 1\n",
    "    print \"true positive rate: \", np.float(true_positive)/len(pred_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(image_tensor,question_tensor):\n",
    "    model = get_model(0.0,'/home/tushar/Desktop/Python/VQA/model_weights.h5')\n",
    "    pred_x = [image_tensor,question_tensor]\n",
    "    preds = model.predict(pred_x)\n",
    "    pred_classes = [np.argmax(_) for _ in value]\n",
    "    pred_classes = pred_classes[0]\n",
    "    metadata = get_metadata()\n",
    "    meta = metadata['ix_to_ans']\n",
    "    for char in meta:\n",
    "        char = int(char)\n",
    "        if pred_classes == char:\n",
    "            char = str(char)\n",
    "            print meta.get(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print \"Creating text model...\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, \n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(units=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_model(dropout_rate):\n",
    "    print \"Creating image model...\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=4096, activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print \"Merging final model...\"\n",
    "    fc_model = Sequential()\n",
    "    fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_align(seq,lengths):\n",
    "    v = np.zeros(np.shape(seq))\n",
    "    N = np.shape(seq)[1]\n",
    "    for i in range(np.shape(seq)[0]):\n",
    "        v[i][N-lengths[i]:N]=seq[i][0:lengths[i]]\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_limit):\n",
    "    print \"Reading Data...\"\n",
    "    img_data = h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/data_img.h5') #data_img\n",
    "    ques_data = h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/data_prepro1.h5') #data_prepro\n",
    "  \n",
    "    img_data = np.array(img_data['images_train'])\n",
    "    img_pos_train = ques_data['img_pos_train'][:data_limit]\n",
    "    train_img_data = np.array([img_data[_-1,:] for _ in img_pos_train])\n",
    "    # Normalizing images\n",
    "    tem = np.sqrt(np.sum(np.multiply(train_img_data, train_img_data), axis=1))\n",
    "    train_img_data = np.divide(train_img_data, np.transpose(np.tile(tem,(4096,1))))\n",
    "\n",
    "    #shifting padding to left side\n",
    "    ques_train = np.array(ques_data['ques_train'])[:data_limit, :]\n",
    "    ques_length_train = np.array(ques_data['ques_length_train'])[:data_limit]\n",
    "    ques_train = right_align(ques_train, ques_length_train)\n",
    "\n",
    "    train_X = [train_img_data, ques_train]\n",
    "    # NOTE should've consturcted one-hots using exhausitve list of answers, cause some answers may not be in dataset\n",
    "    # To temporarily rectify this, all those answer indices is set to 1 in validation set\n",
    "    train_y = to_categorical(ques_data['answers'])[:data_limit, :]\n",
    "\n",
    "    return train_X, train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_val_data():\n",
    "    img_data = h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/data_img.h5')\n",
    "    ques_data = h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/data_prepro.h5')\n",
    "    metadata = get_metadata()\n",
    "    with open('/home/tushar/Desktop/Python/VQA/mscoco_val2014_annotations.json', 'r') as an_file:  #val_annotations_path\n",
    "        annotations = json.loads(an_file.read())\n",
    "\n",
    "    img_data = np.array(img_data['images_test'])\n",
    "    img_pos_train = ques_data['img_pos_test']\n",
    "    train_img_data = np.array([img_data[_-1,:] for _ in img_pos_train])\n",
    "    tem = np.sqrt(np.sum(np.multiply(train_img_data, train_img_data), axis=1))\n",
    "    train_img_data = np.divide(train_img_data, np.transpose(np.tile(tem,(4096,1))))\n",
    "\n",
    "    ques_train = np.array(ques_data['ques_test'])\n",
    "    ques_length_train = np.array(ques_data['ques_length_test'])\n",
    "    ques_train = right_align(ques_train, ques_length_train)\n",
    "\n",
    "    # Convert all last index to 0, coz embeddings were made that way :/\n",
    "    for _ in ques_train:\n",
    "        if 12602 in _:\n",
    "            _[_==12602] = 0\n",
    "\n",
    "    val_X = [train_img_data, ques_train]\n",
    "\n",
    "    ans_to_ix = {str(ans):int(i) for i,ans in metadata['ix_to_ans'].items()}\n",
    "    ques_annotations = {}\n",
    "    for _ in annotations['annotations']:\n",
    "        idx = ans_to_ix.get(_['multiple_choice_answer'].lower())\n",
    "        _['multiple_choice_answer_idx'] = 1 if idx in [None, 1000] else idx\n",
    "        ques_annotations[_['question_id']] = _\n",
    "\n",
    "    abs_val_y = [ques_annotations[ques_id]['multiple_choice_answer_idx'] for ques_id in ques_data['question_id_test']]\n",
    "    abs_val_y = to_categorical(np.array(abs_val_y))\n",
    "\n",
    "    multi_val_y = [list(set([ans_to_ix.get(_['answer'].lower()) for _ in ques_annotations[ques_id]['answers']])) for ques_id in ques_data['question_id_test']]\n",
    "    for i,_ in enumerate(multi_val_y):\n",
    "        multi_val_y[i] = [1 if ans in [None, 1000] else ans for ans in _]\n",
    "\n",
    "    return val_X, abs_val_y, multi_val_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    meta_data = json.load(open('/home/tushar/Desktop/Python/VQA/data_train_val/data_prepro.json', 'r')) #data_prepo_meta\n",
    "    meta_data['ix_to_word'] = {str(word):int(i) for i,word in meta_data['ix_to_word'].items()}\n",
    "    return meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_embeddings(num_words, embedding_dim, metadata):\n",
    "    if os.path.exists('/home/tushar/Desktop/Python/VQA/data_train_val/embedding_matrix.h5'): #embedding_matrix_filename\n",
    "        with h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/embedding_matrix.h5') as f:  #embedding_matrix_filename\n",
    "            return np.array(f['embedding_matrix'])\n",
    "\n",
    "    print \"Embedding Data...\"\n",
    "   # with open('/home/tushar/Desktop/Python/VQA/data_train_val/data_prepro.json', 'r') as qs_file:  #train_questions_path\n",
    "    #    questions = json.loads(qs_file.read())\n",
    "     #   texts = [str(_['question']) for _ in questions['questions']]\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open('/home/tushar/Desktop/Python/VQA/glove.6B/glove.6B.300d.txt', 'r') as glove_file:  #glove path\n",
    "        for line in glove_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    word_index = metadata['ix_to_word']\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "   \n",
    "    with h5py.File('/home/tushar/Desktop/Python/VQA/data_train_val/embedding_matrix.h5', 'w') as f: #embedding_matrix_filename\n",
    "        f.create_dataset('embedding_matrix', data=embedding_matrix)\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def question_to_encode(question):\n",
    "\n",
    "    question=question.split()\n",
    "    \n",
    "    l = np.array(question).size\n",
    "    \n",
    "    arr=np.zeros(26)\n",
    "    metadata = get_metadata()\n",
    "    word_index = metadata['ix_to_word']\n",
    "    \n",
    "    i=0\n",
    "\n",
    "    for char in question:\n",
    "        char = char.lower()\n",
    "        if char in word_index.keys():\n",
    "            arr[i] = word_index.get(char)\n",
    "            i=i+1\n",
    "    \n",
    "    i = 0 \n",
    "    arr1 = np.zeros(26)\n",
    "    while i < l:\n",
    "        arr1[25-l+1+i] = arr[i]\n",
    "        i = i+1\n",
    "    \n",
    "    arr1=arr1.reshape((-1,26))\n",
    "    return arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_extract(img_path):\n",
    "    model = VGG19(weights='imagenet', include_top=True)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    model_extractfeatures = Model(input=model.input, output=model.get_layer('fc2').output)\n",
    "    fc2_features = model_extractfeatures.predict(x)\n",
    "    fc2_features = fc2_features.reshape((-1,4096))\n",
    "    fc2_features = np.array(fc2_features)\n",
    "    return fc2_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
